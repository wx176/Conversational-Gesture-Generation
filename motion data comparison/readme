We conducted a comparative analysis of speech-gesture data obtained from three sources: real videos from the Internet, motion scripts generated by LLMs, and manually designed motion scripts. 

We collected five videos featuring news anchors and museum or exhibition hall guides from the Internet. The audio from these videos was extracted and used as input data. To recreate the gestures observed in the original videos, we employed the speech-gesture data expansion method, manually reproducing the actions using predefined motion units. However, because the motion units were sourced from a predefined library, the body positions and movement speeds did not fully match the original gestures in the videos.

For motion scripts generated by LLM, we focused on two specific roles: news anchor and guide. The prompts provided to the LLM included instructions such as: "Imagine you are an excellent museum guide. Please introduce [specific topic] to the visitors and configure corresponding motions. Refer to the examples below..." Based on these prompts, the LLM generated motion scripts that included accompanying gestures.

For gestures designed entirely by hand, humans fully created the gestures corresponding to the speech. However, the speech content was generated by the LLM.
